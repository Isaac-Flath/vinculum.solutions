{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This file downloads and loads data for irs-filings\n",
    "\n",
    "**Post Description:** Download and process IRS data on political organizations (ie 527)\n",
    "\n",
    "**Post Categories:** Data Engineering | IRS\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    ">Credit: [This repo](https://github.com/sahilchinoy/django-irs-filings) by [Sahil Chinoy](https://sahilchinoy.com/) was very helpful in writing this code.  I changed a lot, but left a lot the same.  The data provided by the irs has changed significantly since the original code was written, but it was a huge help anyway.  Several chunks of the code below is taken close to straight from that repo.\n",
    "\n",
    "https://forms.irs.gov/app/pod/dataDownload/dataDownload"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os, sys, shutil\n",
    "import csv, io, zipfile, pickle\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd \n",
    "from IPython.display import clear_output\n",
    "import sqlite3\n",
    "\n",
    "from fastcore.all import *\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "logger = logging.getLogger(name=\"jupyter\")\n",
    "if len(logger.handlers) == 0: logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def def_value(): return []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Extract\n",
    "\n",
    "This step is pretty straightforward.  We need to download and unzip a file.  When unzipped there is a single file that is almost a half dozen directories deep so we also clean that up for ease of use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Import functions for downloading and unzipping file\n",
    "from download_utils import unzip_file, download_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Flatten the directory tree out\n",
    "def clean_527(zip_path,extract_path,final_path):\n",
    "    logger.info('Cleaning up archive...')\n",
    "    shutil.move(f\"{extract_path}/var/IRS/data/scripts/pofd/download/FullDataFile.txt\",final_path)\n",
    "    shutil.rmtree(extract_path)\n",
    "    os.remove(zip_path)\n",
    "    logger.info(f\"FINAL RAW DATA FILE RELATIVE PATH: {final_path}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Download, unzip, flatten\n",
    "def extract_data(url, zip_path,extract_path,final_path):\n",
    "    download_file(url,zip_path)\n",
    "    unzip_file(zip_path,extract_path)\n",
    "    clean_527(zip_path,extract_path,final_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "url = 'http://forms.irs.gov/app/pod/dataDownload/fullData'\n",
    "base_dir = Path(\"./data/irs-filings\")\n",
    "zip_path = (base_dir/'data.zip')\n",
    "extract_path = (base_dir/'unzipped/')\n",
    "final_path = (base_dir/'raw_FullDataFile.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "extract_data(\n",
    "    url, # URL where the data is located on the IRS Website\n",
    "    zip_path, # The location we will download the data to\n",
    "    extract_path, # The location we will unzip the file to\n",
    "    final_path # The final unzipped file location and name\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean and Split\n",
    "\n",
    "Now that we have the file to read in, it gets a bit trickier.  The file is not in a standard format and we need to parse row by row.  The file is pipe delimited, but each row has different number of fields and belong to a different table.\n",
    "\n",
    "In a normal \"relational\" structure this would be multiple different files and we are going to break it out in that way."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to get and set up our mappings.  This gives us our schema of what all the different types of rows we might encounter and what the names and data types of the fields in that type of row will be."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def load_mapping_file(fname, record_types = [\"1\",\"D\",\"R\",\"E\",\"2\",\"A\",\"B\"]):\n",
    "    mappings = {}\n",
    "    for r in record_types: mappings[r] = pd.read_excel(mappings_path,sheet_name=r)\n",
    "    return mappings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def build_mappings(mapping_file):\n",
    "    record_types = L(mapping_file.keys())\n",
    "    mappings = {}\n",
    "    for record_type in record_types:\n",
    "        cols = L(o for o in mapping_file[record_type].columns)\n",
    "        mapping = {}\n",
    "        for row in mapping_file[record_type].values:\n",
    "            mapping[row[cols.index('position')]] = (row[cols.index('model_name')],row[cols.index('field_type')])\n",
    "        mappings[record_type] = mapping\n",
    "    return mappings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we create functions to parse a given row using the mapping file.  It will determine the row type, look up the mapping for that row, and then set the datatype of each cell appropriately."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def clean_cell(cell, cell_type,NULL_TERMS = ['N/A','NOT APPLICABLE','NA','NONE','NOT APPLICABE','NOT APLICABLE','N A','N-A']):\n",
    "    if cell_type == 'D': cell = datetime.strptime(cell, '%Y-%m-%d %H:%M:%S')\n",
    "    elif cell_type == 'I': cell = int(cell)\n",
    "    elif cell_type == 'N': cell = float(cell)\n",
    "    else:\n",
    "        cell = cell.upper()\n",
    "        if len(cell) > 50: cell = cell[0:50]\n",
    "        if not cell or cell in NULL_TERMS: cell = None\n",
    "    return cell"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def parse_row(row, mapping):\n",
    "    fields = mapping\n",
    "    parsed_row = {}\n",
    "    for i, cell in enumerate(row[0:len(fields)]):\n",
    "        field_name, field_type = fields[i]\n",
    "        parsed_cell = clean_cell(cell, field_type)\n",
    "        parsed_row[field_name] = parsed_cell\n",
    "    return parsed_row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Knowing the length of the file lets us know how much we have left to parse\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f): pass\n",
    "    return i + 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def process_file(final_path,mappings):\n",
    "    with io.open(final_path, 'r', encoding='ISO-8859-1') as raw_file:\n",
    "        reader = csv.reader(raw_file, delimiter='|')\n",
    "\n",
    "        def def_value(): return []\n",
    "        records = defaultdict(def_value)\n",
    "        file_length = file_len(final_path)\n",
    "        start_time = datetime.now()\n",
    "        for i,row in enumerate(reader):\n",
    "            try:\n",
    "                form_type = str(row[0])\n",
    "                if form_type in mappings.keys(): \n",
    "                    parsed_row = parse_row(row, mappings[form_type])\n",
    "                    records[form_type].append(parsed_row)\n",
    "                elif form_type in (\"H\",\"F\"): logger.info(row)\n",
    "            except IndexError:\n",
    "                if row != '\\n': records[\"error_idxs\"].append(i)\n",
    "            if i%10000 ==0:\n",
    "                clear_output(wait=True)\n",
    "                elapsed = datetime.now()-start_time\n",
    "                time_per = elapsed/max(i,1)\n",
    "                logger.info(f\"{i} of {file_length} | {round((i/file_length)*100,2)}% | Elapsed={elapsed} | Time Per={time_per} | Remaining={time_per*(file_length-i)}\")\n",
    "    pickle.dump(dict(records), open('data/irs-filings/processed_lists.pickle', \"wb\" ) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "final_path = (base_dir/'raw_FullDataFile.txt')\n",
    "mappings_path = Path(\"DataConfigs/irs-filings/mappings.xlsx\")\n",
    "mappings = build_mappings(load_mapping_file(mappings_path))\n",
    "process_file(final_path,mappings)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'build_mappings' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tk/r2w0qvjj751gxhj5fqglgrtm0000gn/T/ipykernel_33221/3982573387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfinal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'raw_FullDataFile.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmappings_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataConfigs/irs-filings/mappings.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_mapping_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmappings_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmappings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_mappings' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open('data/irs-filings/processed_lists.pickle', 'rb') as handle:\n",
    "    records = pickle.load(handle)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "errors = L(records['error_idxs'])\n",
    "new_errors = []\n",
    "with open(\"data/irs-filings/raw_FullDataFile.txt\") as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        if i in errors:\n",
    "            if line == '\\n': continue\n",
    "            new_errors.append((i,line))\n",
    "len(new_errors)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "(base_dir/'raw_FullDataFile.txt')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Path('data/irs-filings/raw_FullDataFile.txt')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# conn = sqlite3.connect('./data/irs-filings/db.sqlite')for k in records.keys():\n",
    "#     if k=='error_idxs': continue\n",
    "#     print(f\"type_{k} writings...\")\n",
    "#     pd.DataFrame(records[k]).to_sql(f\"type_{k}\",conn,if_exists=\"replace\",index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# pd.read_sql(f\"select * from type_A limit 25\",conn).head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('political-spending-analysis': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "4a7f1ebfaddf68725d4796cad0ddde8ff8a752ccfc0551e8ea50b332ad07ecb7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}